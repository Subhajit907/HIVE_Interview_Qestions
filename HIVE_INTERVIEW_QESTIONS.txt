1. What is the definition of Hive? What is the present version of Hive?
Ans: Hive is designed for querying and managing only structured data stored in tables.Hive uses an SQL-inspired language, sparing the user from dealing with the complexity of MapReduce programming. It makes learning more accessible by utilizing familiar concepts found in relational databases, such as columns, tables, rows, and schema, etc. Hive is scalable, fast, and uses familiar concepts. Schema gets stored in a database, while processed data goes into a Hadoop Distributed File System (HDFS).Tables and databases get created first; then data gets loaded into the proper tables. Hive supports four file formats: ORC, SEQUENCEFILE, RCFILE (Record Columnar File), and TEXTFILE.
                       Present version of Hive is Hive 4.x.

2. Is Hive suitable to be used for OLTP systems? Why?
Ans: No, it is not suitable for OLTP system since it does not offer insert and update at the row level.

3. How is HIVE different from RDBMS? Does hive support ACID transactions. If not then give the proper reason.
Ans:The main difference between RDBMs database and Hive is Specialization. Hive is similer to traditional. Yes Hive support ACID transaction.

4. Explain the hive architecture and the different components of a Hive
architecture?
Ans: Hive architecture helps in determining the hive Query language and the interaction between the programmer and the Query language using the command line since it is built on top of the Hadoop ecosystem it has frequent interaction with the Hadoop and is, therefore, copes up with both the domain SQL database system and Map-reduce, Its major components are Hive Clients(like JDBC, Thrift API, ODBC Applications, etc.), Hive servers and Hive storage a.k.a meta storage.

Different components of a Hive:
a)Hive clients
b)Hive Services
c)Hive storage (Meta storage)

5. Mention what Hive query processor does? And Mention what are the
components of a Hive query processor?
Ans: Hive provides a SQL dialect known as Hive Query Language abbreviated as HQL to retrieve or modify the data. which is stored in the Hadoop. Apache Hive is an open-source data warehouse system built on top of Hadoop Cluster for querying and analyzing large datasets stored in the Hadoop distributed file system.
The components of a Hive query processor:
1.Logical Plan of Generation.
2.Physical Plan of Generation.
3.Execution Engine.
4.UDF’s and UDAF.
5.Operators.
6.Optimizer.
7.Parser.
8.Semantic Analyzer.
9.Type Checking.

6. What are the three different modes in which we can operate Hive?
Ans: We can run Hive in following modes:

Local mode: In Hive local mode, Map Reduce jobs related to Hive run locally on a user machine. This is the default mode in which Hadoop uses local file system.

Distributed Mode: In this mode, Hive as well as Hadoop is running in a fully distributed mode. 

Pseudo-distributed Mode: This is the mode used by developers to test the code before deploying to production. 

7. Features and Limitations of Hive.
Ans: Featres of Hive :  Hive can support client applications written in PHP, Python, Java, C++ and Ruby. 18. Rule Based Optimizer: Hive has a rule based optimizer for optimizing logical plans. 19. Ad-hoc queries: Hive allows us to run Ad-hoc queries which are the loosely typed command or query whose value depends on some variable for the data analysis.

Limitations of Hive: Does not support OLTP, Hive doesn’t support online trans,
Doesn’t support subqueries.

8. How to create a Database in HIVE?
Ans:
Step 1: Start all the Hadoop daemons.
Step 2: Start Hive shell.
Step 3: Create a database with the name Test. Syntax: CREATE DATABASE <database-name>; Command: create database Test;
Step 4: Check the location /user/hive/warehouse on HDFS to see whether the database directory is made or not. For Hadoop 3 go to http://localhost:9870 and For Hadoop 2 go to http://localhost:50070 to browse the name node.

9. How to create a table in HIVE?
Ans:
Step 1: Start all your Hadoop Daemon
start-dfs.sh                    # this will start namenode, datanode and secondary namenode
start-yarn.sh                   # this will start node manager and resource manager  
jps                             # To check running daemons
Step 2: Launch hive from terminal
hive
Creating Table in Hive
Let’s create a database first so that we can create tables inside it. The command for creating a database is shown below.
Syntax To Make Database:
CREATE DATABASE <database-name>;
Command:
CREATE DATABASE student_detail;   # this will create database student_detail
SHOW DATABASES;                   # list down all the available databases
Now, to have access to this database we have to use it.
Syntax:
USE <database-name>;
Command:USE student_detail;
Syntax To Create Table in Hive

CREATE TABLE [IF NOT EXISTS] <table-name> (
<column-name>    <data-type>,
<column-name>    <data-type> COMMENT 'Your Comment',
<column-name>    <data-type>,
<column-name>    <data-type>
)
COMMENT 'Add if you want'
LOCATION 'Location On HDFS'
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ',';
Note:

1. We can add a comment to the table as well as to each individual column.

2. ROW FORMAT DELIMITED shows that whenever a new line is encountered the new record entry will start. 

3. FIELDS TERMINATED BY ‘,’ shows that we are using ‘,’ delimiter to separate each column. 

4. We can also override the default database location with the LOCATION option.

So let’s create the table student_data in our student_detail database with the help of the command shown below.

CREATE TABLE IF NOT EXISTS student_data(
Student_Name STRING COMMENT 'This col. Store the name of student', 
Student_Rollno INT COMMENT 'This col. Stores the rollno of student',
Student_Marks FLOAT)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','

10.What do you mean by describe and describe extended and describe
formatted with respect to database and table
11.How to skip header rows from a table in Hive?
Ans:As of Hive v0.13.0, you can use skip.header.line.count table property: create external table testtable (name string, message string) row format delimited fields terminated by 't' lines terminated by 'n' location '/testtable' TBLPROPERTIES ("skip.header.line.count"="1"); Use ALTER TABLE for an existing table:

12.What is a hive operator? What are the different types of hive operators?
Ans: This chapter explains the built-in operators of Hive. There are four types of operators in Hive:
1.Relational Operators
2.Arithmetic Operators
3.Logical Operators
4.Complex Operators

13.Explain about the Hive Built-In Functions
Ans:The built-in functions available in Hive. The functions look quite similar to SQL functions, except for their usage.

14. Write hive DDL and DML commands.
Ans: HIVE DDL:
In Hive, CREATE DATABASE statement is used to create a Database, this takes an optional clause IF NOT EXISTS, using this option, it creates only when database not already exists.

If the Database already exists you will get an error Database db_name already exists.
To check if the database already exists before creating, use IF NOT EXISTS clause.
You can change the location of the database using LOCATION clause

CREATE DATABASE IF NOT EXISTS emp;
runcate Table
Truncate table is used to truncate the table meaning it deletes all the contents of the table and the structure of the table


jdbc:hive2://>TRUNCATE TABLE emp;
Alter Table
To rename a Table use ALTER TABLE command.


jdbc:hive2://>ALTER TABLE employee RENAME TO employee2;
Drop Table
Drop Table is used to Drop the table from a Hive database.


jdbc:hive2://>DROP TABLE employee2;
jdbc:hive2://>DROP TABLE emp.employee2;
jdbc:hive2://>DROP TABLE IF EXISTS employee2 PURGE;

HIVE DML: Hive DML (Data Manipulation Language) commands are used to insert, update, retrieve, and delete data from the Hive table once the table and database schema has been defined using Hive DDL commands.

The various Hive DML commands are:
1.LOAD
2.SELECT
3.INSERT
4.DELETE
5.UPDATE
6.EXPORT
7.IMPORT

15.Explain about SORT BY, ORDER BY, DISTRIBUTE BY and
CLUSTER BY in Hive.
Ans: 
SORT BY : The HiveQL SORT BY clause is an alternative of ORDER BY clause. It orders the data within each reducer. Hence, it performs the local ordering, where each reducer's output is sorted separately. It may also give a partially ordered result. In this example, we arrange the data in the sorted order by using SORT BY clause.

DISTRIBUTED BY: DISTRIBUTE BY clause is used to distribute the input rows among reducers. It ensures that all rows for the same key columns are going to the same reducer. So, if we need to partition the data on some key column, we can use the DISTRIBUTE BY clause in the hive queries.

CLUSTER BY: CLUSTER BY is a shortcut for both DISTRIBUTE BYand SORT BY. Hive uses the columns in DISTRIBUTE BY to distribute the rows among the reducers. All rows with the same DISTRIBUTE BY columns will go to the same reducer.

16.Difference between "Internal Table" and "External Table" and Mention
when to choose “Internal Table” and “External Table” in Hive?
Ans: Difference between Internal & External tables :
For Internal Tables-
1.Stored in a directory based on settings in hive.metastore.warehouse.dir, by default internal tables are stored in the following directory “/user/hive/warehouse” you can change it by updating the location in the config file .
2.Deleting the table deletes the metadata and data from master-node and HDFS respectively.
3.Internal table file security is controlled solely via HIVE. Security needs to be managed within HIVE, probably at the schema level (depends on organization).

For External Tables -
1.External table stores files on the HDFS server but tables are not linked to the source file completely.
2.If you delete an external table the file still remains on the HDFS server.
As an example if you create an external table called “table_test” in HIVE using HIVE-QL and link the table to file “file”, then deleting “table_test” from HIVE will not delete “file” from HDFS.
3.External table files are accessible to anyone who has access to HDFS file structure and therefore security needs to be managed at the HDFS file/folder level.
4.Meta data is maintained on master node, and deleting an external table from HIVE only deletes the metadata not the data/file.

17.Where does the data of a Hive table get stored?
Ans: The Data for HIVE is always stored in HDFS. For managed tables the data is stored in hive warehouse by default which is a directory in HDFS. For HIVE External table user can specify the location anywhere in HDFS.

18.Is it possible to change the default location of a managed table?
Ans:Yes, you can do it by using the clause – LOCATION ‘<hdfs_path>’ we can change the default location of a managed table.

19.What is a metastore in Hive? What is the default database provided by
Apache Hive for metastore?
Ans: Metastore is the central repository of Hive Metadata. It is divided into 2 pieces a service and the backing store for the data.
                            By defalt, the metastore service runs in the same JVM as the Hive service and countains an embedded Derby database instance backed by the local disk.

20.Why does Hive not store metadata information in HDFS?
Ans: The metadata is not stored in HDFS, because HDFS read/write operations are time-consuming. As such, Hive stores metadata information in the metastore using RDBMS instead of HDFS. This allows us to achieve low latency and is faster.

21.What is a partition in Hive? And Why do we perform partitioning in Hive?
Ans: Hive organizes tables into partitions. It is a way of dividing a table into related parts based on the values of partitioned columns such as date, city, and department. Using partition, it is easy to query a portion of the data.

partitions are sub-divided into buckets, to provide extra structure to the data that may be used for more efficient querying. Bucketing works based on the value of hash function of some column of a table.

22.What is the difference between dynamic partitioning and static
partitioning?
Ans: Dynamic Partition in Hive:-

1.single insert to partition table is known as dynamic partition

2.Usually dynamic partition load the data from non partitioned table

3.Dynamic Partition takes more time in loading data compared to static partition

4.When you have large data stored in a table then Dynamic partition is suitable.

5.If you want to partition number of column but you don’t know how many columns then also dynamic partition is suitable

6.Dynamic partition there is no required where clause to use limit. we can’t perform alter on Dynamic partition

7.You can perform dynamic partition on hive external table and managed table If you want to use Dynamic partition in hive then mode is in nonstrict mode Here is hive dynamic partition properties you should allow
Example:-
SET hive.exec.dynamic.partition = true;

SET hive.exec.dynamic.partition.mode = nonstrict; 

Static Partition in Hive:-

1.Insert input data files individually into a partition table is Static Partition Usually when loading files (big files) into Hive tables static partitions are preferred

2.Static Partition saves your time in loading data compared to dynamic partition You “statically” add a partition in table and move the file into the partition of the table.

3.We can alter the partition in static partition

4.You can get the partition column value form the filename, day of date etc without reading the whole big file. If you want to use Static partition in hive you should set property

5.set hive.mapred.mode = strict
6.This property set by default in hive-site.xml Static partition is in Strict Mode You should use where clause to use limit in static partition You can perform Static partition on Hive   Manage table or external table.

23.How do you check if a particular partition exists?
Ans: This command lists all the partitions for a table. The general syntax for showing partitions is as follows:

SHOW PARTITIONS [db_name.]table_name [PARTITION(partition_spec)];
Where:
[db_name.]: Is an optional clause. This is used to list partitions of the table from a given database.
[PARTITION(partition_spec)]: Is an optional clause. This is used to list a specific partition of a table.
How to do it…
Use the following commands to show partitions in Hive:

The following command will list all the partitions present in the Sales table:
Show partitions Sales;
The following command will list a specific partition of the Sales table:
Show partitions Sales ...

24.How can you stop a partition form being queried?
Ans:We can use ALTER TABLE table_name ENABLE NO_DROP to prevent a table partition from being dropped.
We can use ALTER TABLE table_name ENABLE OFFLINE to prevent a table partition from

25.Why do we need buckets? How Hive distributes the rows into buckets?
Ans: Hive Bucketing overcomes creating too many directories by specifying the number of buckets you wanted to create (you are in control).
On a larger table, creating a bucketing gives you 2-3x better query performance than a non-bucket table.

26.In Hive, how can you enable buckets?
Ans: By default, the bucket is disabled in Hive. We have to enable it by setting value true to the below property in the hive: set hive.enforce.bucketing=true; (Not needed in Hive 2.x onward) This property will select the number of reducers and the cluster by column automatically based on the table.

27.How does bucketing help in the faster execution of queries?
Ans:The main goal of bucketing is to speed up queries and gain performance improvements. There are two main areas where bucketing can help, the first one is to avoid shuffle in queries with joins and aggregations, the second one is to reduce the I/O with a feature called bucket pruning.

28.How to optimise Hive Performance? Explain in very detail.
Ans:However,  to run queries on petabytes of data we all know that hive is a query language which is similar to SQL built on Hadoop ecosystem. So, there are several Hive optimization techniques to improve its performance which we can implement when we run our hive queries.
a) Tez Execution Engine – Hive Optimization Techniques, to increase the Hive performance of our hive query by using our execution engine as Tez. On defining Tez, it is a new application framework built on Hadoop Yarn.
b)if we use appropriate file format on the basis of data. It will drastically increase our query performance.
Basically, for increasing your query performance ORC file format is best suitable.
c)By Partitioning we can optimise Hive Performance.
d)Bucketing in Hive-  let’s suppose a scenario. At times, there is a huge dataset available. However, after partitioning on a particular field or fields, the partitioned file size doesn’t match with the actual expectation and remains huge.
Still, we want to manage the partition results into different parts. Thus, to solve this issue of partitioning, Hive offers Bucketing concept. Basically,  that allows the user to divide table data sets into more manageable parts.

29. What is the use of Hcatalog?
Ans: HCatalog is built on top of the Hive metastore and incorporates Hive's DDL. HCatalog provides read and write interfaces for Pig and MapReduce and uses Hive's command line interface for issuing data definition and metadata exploration commands.

30. Explain about the different types of join in Hive.
Ans: There are 4 types of join in Hive:
a)Full Join
b)Inner Join
c)Left Join
d)Right Join

31.Is it possible to create a Cartesian join between 2 tables, using Hive?
ans:Yes, Cartesion prodct is a way of joining mltiple tables in which all the rows or tuples from one table are paired with the rows and tuples from another table.

32.Explain the SMB Join in Hive?
Ans:  Sort Merge Bucket (SMB) join in hive is mainly used as there is no limit on file or partition or table join. SMB join can best be used when the tables are large. In SMB join the columns are bucketed and sorted using the join columns. All tables should have the same number of buckets in SMB join.

33.What is the difference between order by and sort by which one we should
use?
Ans:Hive sort by and order by commands are used to fetch data in sorted order.
For Example:
Sort by

hive> SELECT  E.EMP_ID FROM Employee E SORT BY E.empid;
Order by

hive> SELECT  E.EMP_ID FROM Employee E order BY E.empid;


34.What is the usefulness of the DISTRIBUTED BY clause in Hive?
Ans:The number of reducers in Hive is decided by either two properties.

1.hive.exec.reducers.bytes.per.reducer - The default value is 1GB, This makes hive to create one reducer for each 1GB of input table's size.
2.mapred.reduce.tasks - takes an intger value, and those many reducers will be prepared for the job. 

35.How does data transfer happen from HDFS to Hive?
Ans:Moving Data from HDFS to Hive Using an External Table This is the most common way to move data into Hive when the ORC file format is required as the target data format. Then Hive can be used to perform a fast parallel and distributed conversion of your data into ORC. 

36.Wherever (Different Directory) I run the hive query, it creates a new
metastore_db, please explain the reason for it?
Ans:The property of interest here is javax.jdo.option.ConnectionURL. The default value of this property is jdbc:derby:;databaseName=metastore_db;create=true. This value specifies that you will be using embedded derby as your Hive metastore and the location of the metastore is metastore_db. Also the metastore will be created if it doesn't already exist.

37.What will happen in case you have not issued the command: ‘SET
hive.enforce.bucketing=true;’ before bucketing a table in Hive?

38.Can a table be renamed in Hive?
Ans: Yes,You can rename a table in hive sung the following command -
Alter Table table_name RENAME TO new_name.

39.Write a query to insert a new column(new_col INT) into a hive table at a
position before an existing column (x_col)
Ans: create table test1 as SELECT cd_screen_function,
     SUM(access_count) AS max_count,
     MIN(response_time_min) as response_time_min,
     AVG(response_time_avg) as response_time_avg,
     MAX(response_time_max) as response_time_max,
     SUM(response_time_tot) as response_time_tot,
     COUNT(*) as row_count
     FROM sheet WHERE  ts_update BETWEEN unix_timestamp('2012-11-01 00:00:00') AND 
     unix_timestamp('2012-11-30 00:00:00') and cd_office = '016'
     GROUP BY cd_screen_function ORDER BY max_count DESC, cd_screen_function; 

40.What is serde operation in HIVE?
Ans: SerDe is short for Serializer/Deserializer. Hive uses the SerDe interface for IO. The interface handles both serialization and deserialization and also interpreting the results of serialization as individual fields for processing.

A SerDe allows Hive to read in data from a table, and write it back out to HDFS in any custom format. Anyone can write their own SerDe for their own data formats.

See Hive SerDe for an introduction to SerDes.

41.Explain how Hive Deserializes and serialises the data?
Ans: Serialization and deserialization formats are popularly known as SerDes. Hive allows the framework to read or write data in a particular format. These formats parse the structured or unstructured data bytes stored in HDFS in accordance with the schema definition of Hive tables.

42.Write the name of the built-in serde in hive.
Ans: SerDe is a short name for Serializer/Deserializer. In Hive, SerDe is used for input/output interface. For any communication we have to Serialize and Deserialize the data.

43.What is the need of custom Serde?
Ans: Basically, for Serializer/Deserializer in Hive or Hive SerDe (an acronym). However, for the purpose of IO, we use the Hive SerDe interface. Hence, it handles both serialization and deserialization in Hive. Also, interprets the results of serialization as individual fields for processing. In addition, to read in data from a table

44.Can you write the name of a complex data type(collection data types) in
Hive?
Ans: YES

45.Can hive queries be executed from script files? How?
Ans: Yes,
The Hive -f command is used to execute one or more hive queries from a file in batch mode.Instead of enter into the Hive CLI and execute the queries one by one,We can directly execute the set of queries using Hive -f option from the command line itself. Syntax of Hive -f command 1 hive -f <filepath> Example for Hive -f option 1

46.What are the default record and field delimiter used for hive text files?
Ans: What are the default record and field delimiter used for hive text files? The default record delimiter is − n And the filed delimiters are − 001,002,003 What do you mean by schema on read? The schema is validated with the data when reading the data and not enforced when writing data.

47.How do you list all databases in Hive whose name starts with s?
Ans: This will work
hive>show databases like 's*';
It will display all databases whose name start with s.

48.What is the difference between LIKE and RLIKE operators in Hive?
Ans: The difference between Like and Rlike in Hive tags: hive You can use Like and Rlike to make a blur match, which uses SQL wildcards, while rlike uses regular matching.

49.How to change the column data type in Hive?
Ans: It's simple usually to change/modify the exesting table use this syntax in Hive.
ALTER TABLE table_name CHANGE old_col_name new_col_name new_data_type

50.How will you convert the string ’51.2’ to a float value in the particular
column?
Ans:Hive CAST(from_datatype as to_datatype) function is used to convert from one data type to another for example to cast String to Integer(int), String to Bigint, String to Decimal, Decimal to Int data types, and many more. This cast() function is referred to as the type conversion function which is used to convert data types in Hive. 

51.What will be the result when you cast ‘abc’ (string) as INT?

52.What does the following query do?
a. INSERT OVERWRITE TABLE employees
b. PARTITION (country, state)
c. SELECT ..., se.cnty, se.st
d. FROM staged_employees se;

53.Write a query where you can overwrite data in a new table from the
existing table.
54.What is the maximum size of a string data type supported by Hive?
Ans: max length of a STRING is 2GB

Explain how Hive supports binary formats.
55. What File Formats and Applications Does Hive Support?
Ans: Hive supports several file formats:
1.Text File
2.SequenceFile
3.RCFile
4.Avro Files
5.ORC Files
6.Parquet
7.Custom INPUTFORMAT and OUTPUTFORMAT

56.How do ORC format tables help Hive to enhance its performance?
Ans:Hive can store the data in highly efficient manner in the Optimized Row Columnar (ORC) file format. It can ease many Hive file format limitations. Using ORC files can improves the performance when reading, writing, and processing data.

57.How can Hive avoid mapreduce while processing the query?
Ans: To understand the reason, first we need to know what map and reduce phases mean:-

Map: Basically a filter which filters and organizes data in sorted order. For e.g. It will filter col1_name, col2_name from a row in the second query. However in 1st query you are reading every column, no filtering is required. Hence no Map phase

Reduce: Reduce is just summary operation data across the rows. for e.g. sum of a coloumn! In both the queries you don't need any summary data. Hence no reducer.

so, 1st query as no map-reduce, 2nd query has only mappers but no reduces.

58.What is view and indexing in hive?
Ans: Views are similar to tables, which are generated based on the requirements.
1.We can save any result set data as a view in Hive
2.Usage is similar to as views used in SQL
3.All type of DML operations can be performed on a view

59.Can the name of a view be the same as the name of a hive table?
Ans:No, we cannot use same name for a table and view in Hive. So we have to select a unique name for a view in Hive.

60.What types of costs are associated in creating indexes on hive tables?
Ans: Below are some of disadvantages of index on hive tables:
1.The improvement in query speed that an index can provide comes at the cost of additional processing to create the index and disk space to store the index references.
2.Indexes are advised to build on the columns which you use in filter conditions.
3.Building more number of index on same table will degrade the performance of the query

61.Give the command to see the indexes on a table.
Ans: The command you can use to see the indexes of a table is written below: SHOW INDEX ON table_name This command will list all the indexes on any of the columns with this table_name command.

62. Explain the process to access subdirectories recursively in Hive queries.
63.If you run a select * query in Hive, why doesn't it run MapReduce?
64.What are the uses of Hive Explode?
Ans:Explode is a User Defined Table generating Function (UDTF) in Hive. It takes an array (or a map) as an input and outputs the elements of the array (or a map) as separate rows. UDTFs can be used in the SELECT expression list and as a part of LATERAL VIEW.

65. What is the available mechanism for connecting applications when we
run Hive as a server?
66.Can the default location of a managed table be changed in Hive?
67.What is the Hive ObjectInspector function?
68.What is UDF in Hive?
Ans:HIVE UDF (User Defined Functions) allow the user to extend HIVE Query Language. Once the UDF is added in the HIVE script, it works like a normal built-in function. To check which all UDFs are loaded in current hive session, we use SHOW command.

69.Write a query to extract data from hdfs to hive.
Ans: CREATE TABLE data
(
  word STRING,
  count INT,
  text2 STRING
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ",";

LOAD DATA LOCAL INPATH '/wordcount.csv'
OVERWRITE INTO TABLE data;   

select word, count from data where word=="gloubiboulga";

70.What is TextInputFormat and SequenceFileInputFormat in hive.
71.How can you prevent a large job from running for a long time in a hive?

72.When do we use explode in Hive?
Ans: You have your data set as arrays of array and you want to explode your data at first level only, so use LATERAL VIEW explode(colname) to explode at the first level.

Below is the SELECT query with explode():

SELECT col1 FROM sample2 LATERAL VIEW EXPLODE(col) explodeVal AS col1;

73.Can Hive process any type of data formats? Why? Explain in very detail
Ans: Yes, To perform analytical activities needs some data types and data formats to process and retrieve the data. By using Hive we can perform Data analysis. Hive Provides HQL (Hive Query Language) which is similar to SQL.

74.Whenever we run a Hive query, a new metastore_db is created. Why?
75.Can we change the data type of a column in a hive table? Write a
complete query.
76.While loading data into a hive table using the LOAD DATA clause, how
do you specify it is a hdfs file and not a local file ?

77.What is the precedence order in Hive configuration?
Ans:The order of precedence of the config files is as follows (later one has higher precedence) – hive-site.xml -> hivemetastore-site.xml -> hiveserver2-site.xml -> '-hiveconf' commandline parameters. hive-site.xml and hive-default.xml.template

78.Which interface is used for accessing the Hive metastore?
Ans:WebHCat API Another web interface that can be used for Hive commands is WebHCat, a REST API (not a GUI). With WebHCat, applications can make HTTP requests to access the Hive metastore (HCatalog DDL) or to create and queue Hive queries and commands, Pig jobs, and MapReduce or YARN jobs (either standard or streaming).

79.Is it possible to compress json in the Hive external table ?
Ans:NO

80.What is the difference between local and remote metastores?
Ans: Local metastore is a metastore service that runs in the same JVM in which the Hive service is running. Remote Metastore has its own separate JVM which runs with its own JVM process. It can also connect to a separate database running in a separate JVM in the same or separate machine.

81.What is the purpose of archiving tables in Hive?

82.What is DBPROPERTY in Hive?
Ans: By default, the Hive database will be created inside the default warehouse directory i.e /user/hive/warehouse. But if we want we can store the database in some other HDFS location as well bt mentioning the same in the location field. The DB properties are nothing but mentioning the details about the database created by the user.

83.Differentiate between local mode and MapReduce mode in Hive.
Ans:The local mode works on a local file system. The input and output data stored in the local file system. The MapReduce mode is also known as Hadoop Mode. It is the default mode. In this Pig renders Pig Latin into MapReduce jobs and executes them on the cluster.